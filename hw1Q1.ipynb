{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b846d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from sklearn import utils\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "#from fast_ml.model_development import train_valid_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import  keras \n",
    "\n",
    "from keras.layers import Input, Flatten, Dense, BatchNormalization, Dropout\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0c5ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/Vipul Maingi/Downloads/neural/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7417b9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data=[]\n",
    "x=[]   #Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98320cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir(path):\n",
    "\n",
    "    data = np.load(path + i)\n",
    "    real_data.append(data)\n",
    "    name = i.split('_')[-1]\n",
    "    name = name.split('.')[0]\n",
    "    \n",
    "    for j in range(data.shape[0]):\n",
    "        x.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e26c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.concatenate(real_data, axis =0)#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eaa2e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2850976, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a171735",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"alarm clock\", \"ant, bear\", \"bicycle\", \"bowtie\", \"calculator\",\n",
    "\"camel\", \"car\", \"cat\", \"coffee cup\", \"crocodile\", \"crown\", \"dragon\",\n",
    "\"duck\", \"elephant\", \"eye\", \"fan\", \"fish\", \"hand\", \"house\"] #y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47983624",
   "metadata": {},
   "outputs": [],
   "source": [
    "z,x = shuffle(z,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20e3c2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x293a26d5400>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(z[2].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7c5d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = z[:100000]\n",
    "y_train = x[:100000]\n",
    "x_val =  z[100000:105000]\n",
    "y_val =  x[100000:105000]\n",
    "x_test = z[105000:125000]\n",
    "y_test = x[105000:125000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64ab1b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "X_train= scaler.fit_transform(x_train)\n",
    "X_val = scaler.fit_transform(x_val)\n",
    "X_test = scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38ae061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = [\"alarm clock\", \"ant\", \"bear\", \"bicycle\", \"bowtie\", \"calculator\",\n",
    "\"camel\", \"car\", \"cat\", \"coffee cup\", \"crocodile\", \"crown\", \"dragon\",\n",
    "\"duck\", \"elephant\", \"eye\", \"fan\", \"fish\", \"hand\", \"house\"]\n",
    "\n",
    "f= {}\n",
    "for i in range(len(category_names)):\n",
    "    f[category_names[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f94ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= []\n",
    "y_train_new = np.array(a)\n",
    "y_test_new = np.array(a)\n",
    "y_val_new = np.array(a)\n",
    "\n",
    "\n",
    "for k in range(len(y_train)):\n",
    "    y_train_new = np.append(y_train_new, f[y_train[k]])\n",
    "\n",
    "for l in range(len(y_test)):\n",
    "    y_test_new = np.append(y_test_new, f[y_test[l]])\n",
    "    \n",
    "for z in range(len(y_val)):\n",
    "    y_val_new = np.append(y_val_new, f[y_val[z]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43d3bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tf = to_categorical(y_train_new)\n",
    "y_test_tf = to_categorical(y_test_new)\n",
    "y_val_tf = to_categorical(y_val_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c61503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normalized = tf.reshape(X_train, [100000, 28, 28, 1])\n",
    "x_validation_normalized = tf.reshape(X_val, [5000, 28, 28, 1])\n",
    "x_test_normalized = tf.reshape(X_test, [20000, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39fa72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb27604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fff37386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = tf.convert_to_tensor(x_train_normalized)\n",
    "# Y_train = tf.convert_to_tensor(y_train_new)\n",
    "# # X_val =tf.convert_to_tensor(x_validation_normalized)\n",
    "# Y_val = tf.convert_to_tensor(y_val_tf) \n",
    "# # X_test = tf.convert_to_tensor(x_test_normalized)\n",
    "# Y_test = tf.convert_to_tensor(y_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73d4370c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 3, 3, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 128)         147584    \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 3, 3, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 602,100\n",
      "Trainable params: 601,396\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D,MaxPool2D\n",
    "\n",
    "#np.random.seed(13) #To obtain consistent and reproducible data\n",
    "\n",
    "# This is deep convolutional NN with 6 convolutional layers, 3 MaxPool layers\n",
    "# and one dense layer. Run (on Colab with GPU) and see the difference in \n",
    "# performance compared to smaller convolutional model, and the dense model\n",
    "# First, Run this model as is, and examine the test set error rate\n",
    "\n",
    "from functools import partial\n",
    "from keras import regularizers\n",
    "\n",
    "activeList=['relu','elu', 'sigmoid','leaky_relu']\n",
    "act=activeList[0]\n",
    "initialList=['he_normal','he_uniform','glorot_normal','glorot_uniform',\\\n",
    "             'lecun_normal','lecun_uniform']\n",
    "init=initialList[0]\n",
    "optimList=['rmsprop','adam','nadam','sgd']\n",
    "optim=optimList[2]\n",
    "\n",
    "ConvLayer=partial(Conv2D,activation=act, \n",
    "                  kernel_initializer=init, \n",
    "                  kernel_regularizer=regularizers.l2(0.001),\n",
    "                  strides=1,\n",
    "                  padding='same')\n",
    "# First set of two Conv2D and one MaxPool2D layer\n",
    "# Run this first\n",
    "\n",
    "inputLayer=Input(shape=(28,28,1)) \n",
    "tmp=ConvLayer(filters=32,kernel_size=(3,3))(inputLayer)\n",
    "tmp=ConvLayer(filters=32,kernel_size=(3,3))(tmp)\n",
    "tmp=MaxPool2D((2,2))(tmp)\n",
    "tmp=BatchNormalization()(tmp)\n",
    "#tmp=Dropout(p)(tmp)\n",
    "# Second set of two Conv2D and one MaxPool2D layer\n",
    "# Run this Next and observe how much better the test error get, if any\n",
    "tmp=ConvLayer(filters=64,kernel_size=(3,3))(tmp)\n",
    "tmp=ConvLayer(filters=64,kernel_size=(3,3))(tmp)\n",
    "tmp=MaxPool2D((2,2))(tmp)\n",
    "tmp=BatchNormalization()(tmp)\n",
    "#tmp=Dropout(p)(tmp)# Third set of two Conv2D and one MaxPool2D layer\n",
    "# Run this Next and observe how much better the test error get, if any\n",
    "tmp=ConvLayer(filters=128,kernel_size=(3,3))(tmp)\n",
    "tmp=ConvLayer(filters=128,kernel_size=(3,3))(tmp)\n",
    "tmp=MaxPool2D((2,2))(tmp)\n",
    "tmp=BatchNormalization()(tmp)\n",
    "#tmp=Dropout(p)(tmp)\n",
    "# tmp=Flatten()(tmp)\n",
    "tmp=ConvLayer(filters=128,kernel_size=(3,3))(tmp)\n",
    "tmp=ConvLayer(filters=128,kernel_size=(3,3))(tmp)\n",
    "tmp=MaxPool2D((2,2))(tmp)\n",
    "tmp=BatchNormalization()(tmp)\n",
    "tmp=Flatten()(tmp)\n",
    "tmp=Dense(128,activation='relu',kernel_initializer='he_uniform')(tmp)\n",
    "tmp=Dropout(0.4)(tmp)\n",
    "# tmp=Dense(128,activation='relu',kernel_initializer='he_uniform')(tmp)\n",
    "# tmp=Dropout(0.4)(tmp)\n",
    "outputLayer=Dense(units=20,activation='softmax')(tmp)\n",
    "\n",
    "network=Model(inputLayer,outputLayer)\n",
    "\n",
    "network.compile(optimizer=optim,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "network.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9d569ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "782/782 [==============================] - 198s 254ms/step - loss: 2.1281 - accuracy: 0.7464 - val_loss: 1.7744 - val_accuracy: 0.7680\n",
      "Epoch 2/20\n",
      "782/782 [==============================] - 184s 235ms/step - loss: 1.3092 - accuracy: 0.8570 - val_loss: 1.1244 - val_accuracy: 0.8596\n",
      "Epoch 3/20\n",
      "782/782 [==============================] - 182s 233ms/step - loss: 0.9758 - accuracy: 0.8754 - val_loss: 1.2716 - val_accuracy: 0.7740\n",
      "Epoch 4/20\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 0.8213 - accuracy: 0.8817 - val_loss: 1.0319 - val_accuracy: 0.8092\n",
      "Epoch 5/20\n",
      "782/782 [==============================] - 1317s 2s/step - loss: 0.7433 - accuracy: 0.8878 - val_loss: 0.8405 - val_accuracy: 0.8512\n",
      "Epoch 6/20\n",
      "782/782 [==============================] - 229s 293ms/step - loss: 0.7044 - accuracy: 0.8927 - val_loss: 0.7172 - val_accuracy: 0.8834\n",
      "Epoch 7/20\n",
      "782/782 [==============================] - 238s 305ms/step - loss: 0.6782 - accuracy: 0.8956 - val_loss: 0.8733 - val_accuracy: 0.8316\n",
      "Epoch 8/20\n",
      "782/782 [==============================] - 238s 304ms/step - loss: 0.6560 - accuracy: 0.8969 - val_loss: 0.9277 - val_accuracy: 0.8296\n",
      "Epoch 9/20\n",
      "782/782 [==============================] - 242s 310ms/step - loss: 0.6362 - accuracy: 0.9003 - val_loss: 0.7893 - val_accuracy: 0.8548\n",
      "Epoch 10/20\n",
      "782/782 [==============================] - 234s 300ms/step - loss: 0.6170 - accuracy: 0.9036 - val_loss: 0.7166 - val_accuracy: 0.8710\n",
      "Epoch 11/20\n",
      "782/782 [==============================] - 232s 297ms/step - loss: 0.6053 - accuracy: 0.9060 - val_loss: 0.8361 - val_accuracy: 0.8324\n",
      "Epoch 12/20\n",
      "782/782 [==============================] - 237s 303ms/step - loss: 0.5903 - accuracy: 0.9069 - val_loss: 0.6800 - val_accuracy: 0.8778\n",
      "Epoch 13/20\n",
      "782/782 [==============================] - 222s 284ms/step - loss: 0.5804 - accuracy: 0.9093 - val_loss: 0.6757 - val_accuracy: 0.8872\n",
      "Epoch 14/20\n",
      "782/782 [==============================] - 226s 289ms/step - loss: 0.5710 - accuracy: 0.9103 - val_loss: 0.6586 - val_accuracy: 0.8854\n",
      "Epoch 15/20\n",
      "782/782 [==============================] - 190s 242ms/step - loss: 0.5605 - accuracy: 0.9117 - val_loss: 0.7023 - val_accuracy: 0.8712\n",
      "Epoch 16/20\n",
      "782/782 [==============================] - 184s 235ms/step - loss: 0.5514 - accuracy: 0.9127 - val_loss: 0.6755 - val_accuracy: 0.8816\n",
      "Epoch 17/20\n",
      "782/782 [==============================] - 182s 232ms/step - loss: 0.5455 - accuracy: 0.9133 - val_loss: 0.6431 - val_accuracy: 0.8866\n",
      "Epoch 18/20\n",
      "782/782 [==============================] - 171s 218ms/step - loss: 0.5402 - accuracy: 0.9146 - val_loss: 0.6625 - val_accuracy: 0.8864\n",
      "Epoch 19/20\n",
      "782/782 [==============================] - 168s 214ms/step - loss: 0.5318 - accuracy: 0.9156 - val_loss: 0.6409 - val_accuracy: 0.8918\n",
      "Epoch 20/20\n",
      "782/782 [==============================] - 168s 215ms/step - loss: 0.5324 - accuracy: 0.9154 - val_loss: 0.6670 - val_accuracy: 0.8798\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.6719 - accuracy: 0.8791\n",
      "test_acc: 0.879\n"
     ]
    }
   ],
   "source": [
    "earlyStoppingCB=keras.callbacks.EarlyStopping(patience=5,restore_best_weights=True)\n",
    "history=network.fit(x_train_normalized, y_train_tf, validation_data=(x_validation_normalized,y_val_tf),callbacks=[earlyStoppingCB],epochs=20,batch_size=128)\n",
    "test_loss, test_acc=network.evaluate(x_test_normalized, y_test_tf)\n",
    "print(f\"test_acc: {test_acc:5.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00afbe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
